# Base configuration for RL Rocket League Bot

defaults:
  - _self_
  - training: ppo_1v1
  # - rewards: early_stage  # Disabled for ground test

# Environment settings
environment:
  tick_skip: 8  # 15Hz decisions at 120Hz physics
  max_players: 6
  spawn_opponents: true
  team_size: 1
  gravity: -650.0
  boost_consumption: 33.3

  # Fixed episode length: goals reset state but don't end episode
  # Episode ends only at step limit. More goals = more learning signal!
  episode_steps: 512  # ~34 seconds of game time at 15Hz

  # Legacy (only used if episode_steps is None/0)
  terminal_conditions: [goal, timeout, no_touch]
  timeout_seconds: 300
  no_touch_timeout_seconds: 30

# Observation settings
observation:
  # Normalization constants
  pos_norm: [4096.0, 5120.0, 2048.0]  # X, Y, Z
  vel_norm: 2300.0  # Max car speed
  ang_vel_norm: 5.5  # Max angular velocity

  # Feature dimensions (with goal features and velocity magnitudes)
  # self_car: 19 base + 1 speed + 4 goal features = 24
  # ball: 15 base + 1 speed + 3 goal features = 19
  self_car_dim: 24
  other_car_dim: 14
  ball_dim: 19

  # Team-side invariance
  flip_for_orange: true

# Action settings
action:
  # Multi-discrete action space
  # 3^5 (throttle/steer/pitch/yaw/roll) × 2^3 (jump/boost/handbrake) = 1944
  n_actions: 1944

  # Continuous values for each discrete option
  throttle_options: [-1.0, 0.0, 1.0]
  steer_options: [-1.0, 0.0, 1.0]
  pitch_options: [-1.0, 0.0, 1.0]
  yaw_options: [-1.0, 0.0, 1.0]
  roll_options: [-1.0, 0.0, 1.0]

  # Binary options
  jump_options: [0, 1]
  boost_options: [0, 1]
  handbrake_options: [0, 1]

  # Action masking
  mask_jump_in_air: true
  mask_boost_empty: true

  # Multi-discrete: 8 independent heads (21 logits) vs flat (1944 logits)
  use_multi_discrete: true

# Network architecture
network:
  # Encoders
  car_encoder:
    hidden_dims: [256, 256]
    output_dim: 128
    activation: relu

  ball_encoder:
    hidden_dims: [256, 256]
    output_dim: 64
    activation: relu

  # Attention mechanism
  attention:
    n_heads: 4
    n_layers: 3
    embed_dim: 256
    ff_dim: 512
    dropout: 0.0

  # LSTM temporal memory
  lstm:
    use_lstm: true
    hidden_size: 256
    num_layers: 1
    sequence_length: 32

  # Policy head
  policy_head:
    hidden_dims: [512, 512, 256]
    activation: relu

  # Value head
  value_head:
    hidden_dims: [512, 512, 256]
    activation: relu

# PPO hyperparameters
ppo:
  learning_rate: 3.0e-4       # Higher initial LR for faster learning
  lr_end: 1.0e-5
  lr_anneal_steps: 500_000_000  # Slower annealing for long training
  lr_warmup_steps: 3          # Warmup LR over first 3 updates (prevents extreme first-step dynamics)

  # Fixed batch size: total samples per training update
  # This is independent of worker count - more workers = faster collection, same ratio
  # With 256 envs (32 workers × 4 envs × 2 agents): 65536 / 256 = 256 steps/env
  # With 1024 envs (64 workers × 8 envs × 2 agents): 65536 / 1024 = 64 steps/env
  batch_size: 65_536
  minibatch_size: 4096        # Larger minibatches for stability
  experience_buffer_size: 100_000

  gamma: 0.995                # High discount for long-horizon goals
  gae_lambda: 0.95
  clip_epsilon: 0.2

  entropy_coef: 0.001         # Lower entropy to not dominate value loss
  value_coef: 0.5

  n_epochs: 4                 # Fewer epochs with larger batches
  max_grad_norm: 0.5

  normalize_advantages: true

# Training settings
training:
  total_steps: 10_000_000_000
  n_workers: 32
  n_envs_per_worker: 4

  # Checkpointing
  checkpoint_interval: 5_000_000   # Every 5M steps (~300 updates)
  checkpoint_dir: data/checkpoints

  # Evaluation
  eval_interval: 10_000_000        # Every 10M steps
  eval_episodes: 100

  # Logging
  log_interval: 50_000             # Every 50k steps (~3 updates)
  wandb_project: rlbot-competitive
  wandb_entity: null

# Reward settings with smooth weight schedules
# Each reward can be a constant float or [start, end] tuple interpolated over schedule_steps
rewards:
  # Ball interaction - these should dominate early training
  touch_velocity: 1.0                    # High early (learn to touch!), normalize later
  velocity_ball_to_goal: 1.0             # Reward ball moving toward goal
  speed_toward_ball: 0.05                 # High early (learn to reach ball), fade later

  # Scoring
  goal: 50.0                             # Big spike when scoring

  # Resource management
  save_boost: 0.0                        # DISABLED: was dominating without ball interaction

  # Mechanics
  demo: 1.0                              # Constant: moderate demo reward
  aerial_height: 0.0                     # Disabled: no aerial reward

  # Team play (disabled for now)
  team_spacing_penalty: 0.0              # Disabled

  # Grounded reward
  on_ground: 0.01                        # DISABLED: focus on ball first

  # Team spirit schedule
  team_spirit: 0.0
  team_spirit_end: 0.5

  # How many steps to reach end weights
  schedule_steps: 2_000_000_000          # 2B steps for gradual fade

# State mutators
state_mutators:
  kickoff_prob: 0.3
  random_prob: 0.5
  replay_prob: 0.2

# Curriculum settings
curriculum:
  enabled: true
  phases:
    - name: basic_movement
      end_step: 100_000_000
      rewards: [speed_toward_ball, touch_velocity]
      team_size: 1

    - name: scoring
      end_step: 500_000_000
      rewards: [speed_toward_ball, touch_velocity, velocity_ball_to_goal, goal]
      team_size: 1

    - name: advanced
      end_step: 1_000_000_000
      rewards: [touch_velocity, velocity_ball_to_goal, goal, aerial_height, demo]
      team_size: 1

    - name: team_play
      end_step: 5_000_000_000
      rewards: [touch_velocity, velocity_ball_to_goal, goal, team_spacing_penalty]
      team_size: 2
      team_spirit: 0.3

    - name: self_play
      end_step: 10_000_000_000
      rewards: [goal]
      team_size: 3
      team_spirit: 0.5
      use_historical_checkpoints: true

# Device settings
device: auto  # auto, cuda, cpu
seed: 42
deterministic: false
