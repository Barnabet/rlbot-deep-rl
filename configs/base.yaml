# Base configuration for RL Rocket League Bot

defaults:
  - _self_
  - training: ppo_1v1
  - rewards: early_stage

# Environment settings
environment:
  tick_skip: 8  # 15Hz decisions at 120Hz physics
  max_players: 6
  spawn_opponents: true
  team_size: 1
  gravity: -650.0
  boost_consumption: 33.3
  terminal_conditions:
    - goal
    - timeout
    - no_touch
  timeout_seconds: 300
  no_touch_timeout_seconds: 30

# Observation settings
observation:
  # Normalization constants
  pos_norm: [4096.0, 5120.0, 2048.0]  # X, Y, Z
  vel_norm: 2300.0  # Max car speed
  ang_vel_norm: 5.5  # Max angular velocity

  # Feature dimensions
  self_car_dim: 19
  other_car_dim: 14
  ball_dim: 15

  # Team-side invariance
  flip_for_orange: true

# Action settings
action:
  # Multi-discrete action space
  # 3^5 (throttle/steer/pitch/yaw/roll) Ã— 2^3 (jump/boost/handbrake) = 1944
  n_actions: 1944

  # Continuous values for each discrete option
  throttle_options: [-1.0, 0.0, 1.0]
  steer_options: [-1.0, 0.0, 1.0]
  pitch_options: [-1.0, 0.0, 1.0]
  yaw_options: [-1.0, 0.0, 1.0]
  roll_options: [-1.0, 0.0, 1.0]

  # Binary options
  jump_options: [0, 1]
  boost_options: [0, 1]
  handbrake_options: [0, 1]

  # Action masking
  mask_jump_in_air: true
  mask_boost_empty: true

# Network architecture
network:
  # Encoders
  car_encoder:
    hidden_dims: [256, 256]
    output_dim: 128
    activation: relu

  ball_encoder:
    hidden_dims: [256, 256]
    output_dim: 64
    activation: relu

  # Attention mechanism
  attention:
    n_heads: 4
    n_layers: 2
    embed_dim: 128
    ff_dim: 512
    dropout: 0.0

  # Policy head
  policy_head:
    hidden_dims: [512, 512, 256]
    activation: relu

  # Value head
  value_head:
    hidden_dims: [512, 512, 256]
    activation: relu

# PPO hyperparameters
ppo:
  learning_rate: 1.0e-4
  lr_end: 1.0e-5
  lr_anneal_steps: 100_000_000

  batch_size: 100_000
  minibatch_size: 25_000
  experience_buffer_size: 200_000

  gamma: 0.995
  gae_lambda: 0.95
  clip_epsilon: 0.2

  entropy_coef: 0.01
  value_coef: 0.5

  n_epochs: 3
  max_grad_norm: 0.5

  normalize_advantages: true

# Training settings
training:
  total_steps: 10_000_000_000
  n_workers: 32
  n_envs_per_worker: 4

  # Checkpointing
  checkpoint_interval: 10_000_000
  checkpoint_dir: data/checkpoints

  # Evaluation
  eval_interval: 5_000_000
  eval_episodes: 100

  # Logging
  log_interval: 10_000
  wandb_project: rlbot-competitive
  wandb_entity: null

# Reward settings (can be overridden by reward config)
rewards:
  # Base reward weights
  touch_velocity: 50.0
  velocity_ball_to_goal: 10.0
  speed_toward_ball: 1.0
  goal: 100.0
  save_boost: 2.0
  demo: 5.0
  aerial_height: 0.5
  team_spacing_penalty: -0.1

  # Annealing
  anneal_steps: 500_000_000
  team_spirit: 0.0
  team_spirit_end: 0.3

# State mutators
state_mutators:
  kickoff_prob: 0.3
  random_prob: 0.5
  replay_prob: 0.2

# Curriculum settings
curriculum:
  enabled: true
  phases:
    - name: basic_movement
      end_step: 100_000_000
      rewards: [speed_toward_ball, touch_velocity]
      team_size: 1

    - name: scoring
      end_step: 500_000_000
      rewards: [speed_toward_ball, touch_velocity, velocity_ball_to_goal, goal]
      team_size: 1

    - name: advanced
      end_step: 1_000_000_000
      rewards: [touch_velocity, velocity_ball_to_goal, goal, aerial_height, demo]
      team_size: 1

    - name: team_play
      end_step: 5_000_000_000
      rewards: [touch_velocity, velocity_ball_to_goal, goal, team_spacing_penalty]
      team_size: 2
      team_spirit: 0.3

    - name: self_play
      end_step: 10_000_000_000
      rewards: [goal]
      team_size: 3
      team_spirit: 0.5
      use_historical_checkpoints: true

# Device settings
device: auto  # auto, cuda, cpu
seed: 42
deterministic: false
